# attention-branch-network

Attention Branch Networkï¼ˆABNï¼‰ã®å®Ÿè£…ã§ã™ã€‚`torchvision.datasets.Imagenette`ï¼ˆ10ã‚¯ãƒ©ã‚¹ï¼‰ã‚’ç”¨ã„ãŸç”»åƒåˆ†é¡ã«é©ç”¨ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã“ã‚’è¦‹ã¦äºˆæ¸¬ã—ãŸã‹ã‚’å¯è¦–åŒ–ã§ãã¾ã™ã€‚

![Attention Maps](outputs/abn_attentions.png)

## æ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ ABN ã‚’ ResNet ç³»ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ä¸Šã«å®Ÿè£…ã—ã€Imagenette ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ`full` / `320px` / `160px`ï¼‰ã§ã®å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»å¯è¦–åŒ–ã‚’è¡Œã„ã¾ã™ã€‚å­¦ç¿’ã«ã¯ Hugging Face `Trainer` ã‚’ç”¨ã„ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚’ç°¡æ½”ã«æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚

## è¨“ç·´çµæœ

ResNet152 + ABN ã§ã® Imagenette 10ã‚¯ãƒ©ã‚¹åˆ†é¡ã®çµæœ:

- **Top-1 Accuracy**: 0.8540
- **Top-5 Accuracy**: 0.9758
- **Training Epochs**: 90 epochs

## å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒHugging Face Hubã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ï¼ˆImagenette 10ã‚¯ãƒ©ã‚¹å‘ã‘ãƒ¢ãƒ‡ãƒ«ã€å…¬é–‹æº–å‚™ä¸­ï¼‰ï¼š

**ğŸ”— [yukiharada1228/abn-resnet152-imagenette](https://huggingface.co/yukiharada1228/abn-resnet152-imagenette)**

### ãƒ¢ãƒ‡ãƒ«ä»•æ§˜
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ResNet152 + Attention Branch Network
- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: imagenette (10ã‚¯ãƒ©ã‚¹)
- **æ€§èƒ½**: å­¦ç¿’å®Œäº†å¾Œã«æ›´æ–°äºˆå®š
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: Safetensors

### å¯è¦–åŒ–

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸå¯è¦–åŒ–ï¼š

```bash
uv run visualize.py -c yukiharada1228/abn-resnet152-imagenette
```

## ä¸»ãªæ©Ÿèƒ½

- **Imagenette 10ã‚¯ãƒ©ã‚¹åˆ†é¡**: `torchvision.datasets.Imagenette`ã‚’åˆ©ç”¨
- **æ³¨æ„æ©Ÿæ§‹ã®å¯è¦–åŒ–**: åŸç”»åƒã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—é‡ç•³ã‚’æ¨ªä¸¦ã³ãƒšã‚¢ã§ã‚°ãƒªãƒƒãƒ‰ä¿å­˜ï¼ˆæŒ‡å®šã—ãŸã‚¯ãƒ©ã‚¹æ•°åˆ†ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤ºï¼‰
- **ç”»åƒå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«**: `image_processing_abn.py` ã§ç”»åƒã®å‰å‡¦ç†ãƒ»å¾Œå‡¦ç†ã‚’çµ±åˆç®¡ç†
- **Imagenetteãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£**: `imagenette_utils.py` ã«å…±é€šãƒ©ãƒƒãƒ‘ãƒ¼ã¨ã‚¯ãƒ©ã‚¹åæ•´å½¢å‡¦ç†ã‚’é›†ç´„
- **è¤‡æ•°ã® ResNet å¯¾å¿œ**: ResNet18/34/50/101/152
- **Trainer é€£æº**: æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã«å¯¾å¿œ
- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆäº’æ›**: `model.safetensors` ã‹ã‚‰å¯è¦–åŒ–å¯èƒ½

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
attention-branch-network/
â”œâ”€â”€ abn/                    # ABN ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼ˆHFäº’æ›ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ configuration_abn.py
â”‚   â”œâ”€â”€ image_processing_abn.py
â”‚   â”œâ”€â”€ modeling_abn.py
â”‚   â””â”€â”€ resnet_abn_backbone.py
â”œâ”€â”€ checkpoint/            # Trainer å‡ºåŠ›ï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚„ epoch ã”ã¨ã® ckptï¼‰
â”‚   â””â”€â”€ runs/              # TensorBoard äº’æ›ãƒ­ã‚°
â”œâ”€â”€ outputs/               # å¯è¦–åŒ–çµæœï¼ˆã¾ã¨ã‚ç”»åƒï¼‰
â”‚   â””â”€â”€ abn_attentions.png
â”œâ”€â”€ imagenette_utils.py    # Imagenetteç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ train.py               # å­¦ç¿’ãƒ»è©•ä¾¡ï¼ˆHF Trainerï¼‰
â”œâ”€â”€ visualize.py           # æ³¨æ„ãƒãƒƒãƒ—å¯è¦–åŒ–
â”œâ”€â”€ demo.ipynb             # Jupyter ãƒ‡ãƒ¢ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
â”œâ”€â”€ main.py                # ã‚¨ãƒ³ãƒˆãƒªï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
â”œâ”€â”€ pyproject.toml         # ä¾å­˜é–¢ä¿‚ï¼ˆuv å¯¾å¿œï¼‰
â”œâ”€â”€ uv.lock
â”œâ”€â”€ LICENSE
â””â”€â”€ NOTICE.txt
```

## å‹•ä½œç’°å¢ƒ

- Python 3.12 ä»¥ä¸Š
- CUDA ç’°å¢ƒï¼ˆGPU æ¨å¥¨ã€‚`--cpu` ã§CPUå®Ÿè¡Œå¯ï¼‰

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# uv ã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
uv sync
```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆImagenetteï¼‰

`train.py` / `visualize.py` ã¯åˆå›å®Ÿè¡Œæ™‚ã« Imagenette ã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: `torchvision.datasets.Imagenette`
- ã‚¯ãƒ©ã‚¹æ•°: 10ã‚¯ãƒ©ã‚¹ï¼ˆImageNetã®ã‚µãƒ–ã‚»ãƒƒãƒˆï¼‰
- ã‚µã‚¤ã‚º: `full`ï¼ˆæ—¢å®šï¼‰ã€`320px`ã€`160px`ã‚’é¸æŠå¯èƒ½ï¼ˆ`--imagenette-size`ï¼‰
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: æ—¢å®šã§ `data/imagenette` ã«ä¿å­˜ï¼ˆ`--data-root` ã§å¤‰æ›´å¯èƒ½ï¼‰
- åˆ†å‰²: trainï¼ˆå­¦ç¿’ç”¨ï¼‰ã€valï¼ˆè©•ä¾¡ç”¨ï¼‰

## ä½¿ã„æ–¹

### å­¦ç¿’

```bash
uv run train.py
```

#### è©•ä¾¡ã®ã¿

```bash
uv run train.py --evaluate --checkpoint checkpoint
```

### å¯è¦–åŒ–ï¼ˆæ³¨æ„ãƒãƒƒãƒ—ï¼‰

```bash
uv run visualize.py
```

## å¯è¦–åŒ–çµæœãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

- `outputs/{prefix}_attentions.png` ã«ã€åŸç”»åƒã¨é‡ç•³ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ãƒšã‚¢ã‚’ã‚¿ã‚¤ãƒ«é…ç½®ã§ä¿å­˜ã—ã¾ã™ï¼ˆæ—¢å®š: `abn_attentions.png`ï¼‰ã€‚

å®Ÿè£…ã®è¦ç‚¹ï¼ˆABN è«–æ–‡å®Ÿè£…ã«æº–æ‹ ã—ã¤ã¤ç°¡æ½”ãƒ»é«˜é€ŸåŒ–ï¼‰:

1. ç”»åƒå¾©å…ƒ: ImageNet çµ±è¨ˆã§ã®æ­£è¦åŒ–ã‚’åè»¢ã—ã€RGBâ†’BGR ã«å¤‰æ›
2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: `attention[0]` ã‚’ å…¥åŠ›è§£åƒåº¦ã¸ `cv2.resize`
3. ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—: `cv2.COLORMAP_JET` ã‚’é©ç”¨
4. åˆæˆ: `cv2.add(original_bgr, jet_map)`ã€‚`--attention-alpha` ã§å¼·åº¦èª¿æ•´ï¼ˆ1.0 ã§å˜ç´”åŠ ç®—ï¼‰
5. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ: æŒ‡å®šã—ãŸã‚¯ãƒ©ã‚¹æ•°åˆ†ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºã—ã€å·¦ã«åŸç”»åƒãƒ»å³ã«é‡ç•³ç”»åƒã®ãƒšã‚¢ã‚’ã‚¿ã‚¤ãƒ«é…ç½®
6. è¡¨ç¤º: æ—¢å®šã§è¡¨ç¤ºã€`--no-display` ã§ä¿å­˜ã®ã¿

## å¯¾å¿œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

- ResNet18
- ResNet34
- ResNet50
- ResNet101
- ResNet152

## ä¾å­˜é–¢ä¿‚

- PyTorch / torchvision
- Transformers / Accelerate
- NumPy
- Matplotlibï¼ˆå¯è¦–åŒ–ï¼‰
- OpenCVï¼ˆç”»åƒå‡¦ç†ï¼‰
- TensorBoardXï¼ˆãƒ­ã‚°å‡ºåŠ›ï¼‰

`pyproject.toml` ã«å®šç¾©æ¸ˆã¿ã§ã™ã€‚`uv sync` ã§ç’°å¢ƒæ§‹ç¯‰ã§ãã¾ã™ã€‚

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã® `LICENSE` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## Acknowledgements

This project includes code from:
"Attention Branch Network: Learning of Attention Mechanism for Visual Explanation"  
by Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi,  
licensed under the MIT License.  
Original repository: [https://github.com/machine-perception-robotics-group/attention_branch_network](https://github.com/machine-perception-robotics-group/attention_branch_network)

## Citation
If you find this repository is useful. Please cite the following references.

```bibtex
@article{fukui2018cvpr,
    author = {Hiroshi Fukui and Tsubasa Hirakawa and Takayoshi Yamashita and Hironobu Fujiyoshi},
    title = {Attention Branch Network: Learning of Attention Mechanism for Visual Explanation},
    journal = {Computer Vision and Pattern Recognition},
    year = {2019},
    pages = {10705-10714}
}
```

```bibtex
@article{fukui2018arxiv,
    author = {Hiroshi Fukui and Tsubasa Hirakawa and Takayoshi Yamashita and Hironobu Fujiyoshi},
    title = {Attention Branch Network: Learning of Attention Mechanism for Visual Explanation},
    journal = {arXiv preprint arXiv:1812.10025},
    year = {2018}
}  
```
