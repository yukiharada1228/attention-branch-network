# attention-branch-network

Attention Branch Networkï¼ˆABNï¼‰ã®å®Ÿè£…ã§ã™ã€‚ImageNet-1kï¼ˆ1000ã‚¯ãƒ©ã‚¹ï¼‰ã‚’ç”¨ã„ãŸç”»åƒåˆ†é¡ã«é©ç”¨ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã“ã‚’è¦‹ã¦äºˆæ¸¬ã—ãŸã‹ã‚’å¯è¦–åŒ–ã§ãã¾ã™ã€‚

![Attention Maps](outputs/abn_attentions.png)

## æ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ ABN ã‚’ ResNet ç³»ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ä¸Šã«å®Ÿè£…ã—ã€ImageNet-1k ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’ãƒ»è©•ä¾¡ãƒ»å¯è¦–åŒ–ã‚’è¡Œã„ã¾ã™ã€‚å­¦ç¿’ã«ã¯ Hugging Face `Trainer` ã‚’ç”¨ã„ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚’ç°¡æ½”ã«æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚

## DeepWiki

https://deepwiki.com/yukiharada1228/attention-branch-network

## è¨“ç·´çµæœ

ResNet152 + ABN ã§ã® ImageNet-1k 1000ã‚¯ãƒ©ã‚¹åˆ†é¡ã®çµæœ:

- **Top-1 Accuracy**: å­¦ç¿’ä¸­ï¼ˆçµæœã¯å­¦ç¿’å®Œäº†å¾Œã«æ›´æ–°äºˆå®šï¼‰
- **Top-5 Accuracy**: å­¦ç¿’ä¸­ï¼ˆçµæœã¯å­¦ç¿’å®Œäº†å¾Œã«æ›´æ–°äºˆå®šï¼‰
- **Validation Loss**: å­¦ç¿’ä¸­ï¼ˆçµæœã¯å­¦ç¿’å®Œäº†å¾Œã«æ›´æ–°äºˆå®šï¼‰
- **Training Epochs**: 90 epochsï¼ˆæ—¢å®šå€¤ï¼‰

## å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒHugging Face Hubã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ï¼š

**ğŸ”— [yukiharada1228/abn-resnet152](https://huggingface.co/yukiharada1228/abn-resnet152)**ï¼ˆå­¦ç¿’å®Œäº†å¾Œã«å…¬é–‹äºˆå®šï¼‰

### ãƒ¢ãƒ‡ãƒ«ä»•æ§˜
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ResNet152 + Attention Branch Network
- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: ImageNet-1k (1000ã‚¯ãƒ©ã‚¹)
- **æ€§èƒ½**: å­¦ç¿’å®Œäº†å¾Œã«æ›´æ–°äºˆå®š
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: Safetensors

### ä½¿ç”¨æ–¹æ³•

```python
from transformers import AutoModel, AutoImageProcessor
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

processor = AutoImageProcessor.from_pretrained(
    "yukiharada1228/abn-resnet152",
    trust_remote_code=True,
)
model = AutoModel.from_pretrained(
    "yukiharada1228/abn-resnet152",
    trust_remote_code=True,
)

inputs = processor(images=image, return_tensors="pt")

# æ¨è«–
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs["per_logits"]  # äºˆæ¸¬ç”¨ã®logits
    attention_map = outputs["att_map"]  # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ— (B,1,H,W)

# ãƒ¢ãƒ‡ãƒ«ã¯1000ã‚¯ãƒ©ã‚¹ã®ImageNetã„ãšã‚Œã‹ã‚’äºˆæ¸¬ã—ã¾ã™
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])

# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã®å¯è¦–åŒ–
import cv2
import matplotlib.pyplot as plt
import numpy as np

# ç”»åƒã®å‰å‡¦ç†ï¼ˆBGRå½¢å¼ã«å¤‰æ›ï¼‰
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
img_tensor = inputs["pixel_values"][0]
img_rgb = img_tensor.cpu().numpy().transpose((1, 2, 0))
img_rgb = (img_rgb * np.array(std) + np.array(mean)) * 255.0  # æ­£è¦åŒ–ã‚’å…ƒã®ç”»åƒã‚¹ã‚±ãƒ¼ãƒ«(RGB,0-255)ã«æˆ»ã™
img_rgb = np.clip(img_rgb, 0, 255).astype(np.uint8)
img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)

# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã‚’ç”»åƒã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
att_map = attention_map[0, 0].cpu().numpy()  # (H, W)
h, w = img_bgr.shape[:2]
att_resized = cv2.resize(att_map, (w, h))

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
att_scaled = (att_resized * 255.0).astype(np.uint8)
jet_map = cv2.applyColorMap(att_scaled, cv2.COLORMAP_JET)

# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’åŸç”»åƒã«é‡ã­åˆã‚ã›
overlay = cv2.add(img_bgr, jet_map)

# å¯è¦–åŒ–
plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))
plt.title("å¯è¦–åŒ–:ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³Ã—åŸç”»åƒ")
plt.axis('off')
plt.show()
```

### å¯è¦–åŒ–

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸå¯è¦–åŒ–ï¼š

```bash
uv run visualize.py --ckpt yukiharada1228/abn-resnet152 --out-dir outputs --prefix abn
```

## ä¸»ãªæ©Ÿèƒ½

- **ImageNet-1k 1000ã‚¯ãƒ©ã‚¹åˆ†é¡**: Hugging Face datasetsã‹ã‚‰è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- **æ³¨æ„æ©Ÿæ§‹ã®å¯è¦–åŒ–**: åŸç”»åƒã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—é‡ç•³ã‚’æ¨ªä¸¦ã³ãƒšã‚¢ã§ã‚°ãƒªãƒƒãƒ‰ä¿å­˜ï¼ˆæŒ‡å®šã—ãŸã‚¯ãƒ©ã‚¹æ•°åˆ†ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤ºï¼‰
- **ç”»åƒå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«**: `image_processing_abn.py` ã§ç”»åƒã®å‰å‡¦ç†ãƒ»å¾Œå‡¦ç†ã‚’çµ±åˆç®¡ç†
- **è¤‡æ•°ã® ResNet å¯¾å¿œ**: ResNet18/34/50/101/152
- **Trainer é€£æº**: æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã«å¯¾å¿œ
- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆäº’æ›**: `model.safetensors` ã¨ `checkpoint-XXXX` ã®ã©ã¡ã‚‰ã‹ã‚‰ã§ã‚‚å¯è¦–åŒ–å¯èƒ½

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
attention-branch-network/
â”œâ”€â”€ abn/                    # ABN ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼ˆHFäº’æ›ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ configuration_abn.py
â”‚   â”œâ”€â”€ image_processing_abn.py
â”‚   â”œâ”€â”€ modeling_abn.py
â”‚   â””â”€â”€ resnet_abn_backbone.py
â”œâ”€â”€ checkpoint/            # Trainer å‡ºåŠ›ï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚„ epoch ã”ã¨ã® ckptï¼‰
â”‚   â””â”€â”€ runs/              # TensorBoard äº’æ›ãƒ­ã‚°
â”œâ”€â”€ outputs/               # å¯è¦–åŒ–çµæœï¼ˆã¾ã¨ã‚ç”»åƒï¼‰
â”‚   â””â”€â”€ abn_attentions.png
â”œâ”€â”€ train.py               # å­¦ç¿’ãƒ»è©•ä¾¡ï¼ˆHF Trainerï¼‰
â”œâ”€â”€ visualize.py           # æ³¨æ„ãƒãƒƒãƒ—å¯è¦–åŒ–
â”œâ”€â”€ demo.ipynb             # Jupyter ãƒ‡ãƒ¢ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
â”œâ”€â”€ main.py                # ã‚¨ãƒ³ãƒˆãƒªï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
â”œâ”€â”€ pyproject.toml         # ä¾å­˜é–¢ä¿‚ï¼ˆuv å¯¾å¿œï¼‰
â”œâ”€â”€ uv.lock
â”œâ”€â”€ LICENSE
â””â”€â”€ NOTICE.txt
```

## å‹•ä½œç’°å¢ƒ

- Python 3.12 ä»¥ä¸Š
- CUDA ç’°å¢ƒï¼ˆGPU æ¨å¥¨ã€‚`--cpu` ã§CPUå®Ÿè¡Œå¯ï¼‰

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# uv ã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
uv sync
```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆImageNet-1kï¼‰

`train.py`/`visualize.py` ã¯åˆå›å®Ÿè¡Œæ™‚ã« ImageNet-1k ã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: Hugging Face datasets (`ILSVRC/imagenet-1k`)
- ã‚¯ãƒ©ã‚¹æ•°: 1000ã‚¯ãƒ©ã‚¹
- åˆ†å‰²: trainï¼ˆå­¦ç¿’ç”¨ï¼‰ã€validationï¼ˆè©•ä¾¡ç”¨ï¼‰

## ä½¿ã„æ–¹

### å­¦ç¿’

```bash
uv run train.py
```

- æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã¯ `--checkpoint` ã§æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã™ï¼ˆä¾‹: `checkpoint/model.safetensors`ï¼‰ã€‚
- å­¦ç¿’é€”ä¸­ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ `checkpoint-XXXX/` å½¢å¼ã§ä¿å­˜ã•ã‚Œã¾ã™ã€‚
- ãƒ­ã‚°: TensorBoard äº’æ›ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ `checkpoint/runs/` ã«å‡ºåŠ›ã—ã¾ã™ã€‚

#### è©•ä¾¡ã®ã¿

```bash
uv run train.py --evaluate --checkpoint checkpoint --gpu-id 0
```

### å¯è¦–åŒ–ï¼ˆæ³¨æ„ãƒãƒƒãƒ—ï¼‰

æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼ˆ`checkpoint/model.safetensors`ï¼‰ã¾ãŸã¯ä»»æ„ã® `checkpoint-XXXX/` ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

```bash
# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¯è¦–åŒ–ï¼ˆæ—¢å®šãƒ‘ã‚¹ï¼‰
uv run visualize.py --ckpt checkpoint/model.safetensors --out-dir outputs --prefix abn

# ã‚ã‚‹ã‚¨ãƒãƒƒã‚¯ã® ckpt ã‚’æŒ‡å®š
uv run visualize.py --ckpt checkpoint/checkpoint-1924 --out-dir outputs --prefix abn
```

ä¸»ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³:

- å­¦ç¿’ï¼ˆtrain.pyï¼‰
  - `--arch {resnet18,resnet34,resnet50,resnet101,resnet152}`ï¼ˆæ—¢å®š: `resnet152`ï¼‰
  - `-j/--workers`ï¼ˆæ—¢å®š: 4ï¼‰
  - `--train-batch`ï¼ˆæ—¢å®š: 64ï¼‰/`--test-batch`ï¼ˆæ—¢å®š: 100ï¼‰
  - `--epochs`ï¼ˆæ—¢å®š: 90ï¼‰/`--lr`ï¼ˆæ—¢å®š: 0.1ï¼‰/`--momentum`ï¼ˆæ—¢å®š: 0.9ï¼‰/`--wd`ï¼ˆæ—¢å®š: 1e-4ï¼‰
  - `--schedule`ï¼ˆæ—¢å®š: `31 61`ï¼‰/`--gamma`ï¼ˆæ—¢å®š: 0.1ï¼‰
  - `--checkpoint`ï¼ˆå‡ºåŠ›å…ˆã€æ—¢å®š: `checkpoint`ï¼‰/`--resume`ï¼ˆå­¦ç¿’å†é–‹ï¼‰
  - `--evaluate`ï¼ˆè©•ä¾¡ã®ã¿ï¼‰/`--gpu-id`ï¼ˆCUDA ãƒ‡ãƒã‚¤ã‚¹æŒ‡å®šï¼‰/`--push-to-hub`ï¼ˆä»»æ„ï¼‰

- å¯è¦–åŒ–ï¼ˆvisualize.pyï¼‰
  - `--ckpt`ï¼ˆæ—¢å®š: `checkpoint/model.safetensors`ã€‚`checkpoint-XXXX/` ã‚‚å¯ï¼‰
  - `--out-dir`ï¼ˆæ—¢å®š: `outputs`ï¼‰/`--prefix`ï¼ˆæ—¢å®š: `abn`ï¼‰/`--dpi`ï¼ˆæ—¢å®š: 200ï¼‰
  - `--attention-alpha`ï¼ˆ0.0â€“1.0ã€æ—¢å®š: 1.0ã€‚1.0ã§å˜ç´”åŠ ç®—ï¼‰/`--no-display`
  - `--num-classes`ï¼ˆè¡¨ç¤ºã™ã‚‹ã‚¯ãƒ©ã‚¹æ•°ã€æ—¢å®š: 10ï¼‰/`--arch` / `-j/--workers` / `--gpu-id` ã¾ãŸã¯ `--cpu`

## å¯è¦–åŒ–çµæœãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

- `outputs/{prefix}_attentions.png` ã«ã€åŸç”»åƒã¨é‡ç•³ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ãƒšã‚¢ã‚’ã‚¿ã‚¤ãƒ«é…ç½®ã§ä¿å­˜ã—ã¾ã™ï¼ˆæ—¢å®š: `abn_attentions.png`ï¼‰ã€‚

å®Ÿè£…ã®è¦ç‚¹ï¼ˆABN è«–æ–‡å®Ÿè£…ã«æº–æ‹ ã—ã¤ã¤ç°¡æ½”ãƒ»é«˜é€ŸåŒ–ï¼‰:

1. ç”»åƒå¾©å…ƒ: ImageNet çµ±è¨ˆã§ã®æ­£è¦åŒ–ã‚’åè»¢ã—ã€RGBâ†’BGR ã«å¤‰æ›
2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: `attention[0]` ã‚’ å…¥åŠ›è§£åƒåº¦ã¸ `cv2.resize`
3. ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—: `cv2.COLORMAP_JET` ã‚’é©ç”¨
4. åˆæˆ: `cv2.add(original_bgr, jet_map)`ã€‚`--attention-alpha` ã§å¼·åº¦èª¿æ•´ï¼ˆ1.0 ã§å˜ç´”åŠ ç®—ï¼‰
5. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ: æŒ‡å®šã—ãŸã‚¯ãƒ©ã‚¹æ•°åˆ†ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºã—ã€å·¦ã«åŸç”»åƒãƒ»å³ã«é‡ç•³ç”»åƒã®ãƒšã‚¢ã‚’ã‚¿ã‚¤ãƒ«é…ç½®
6. è¡¨ç¤º: æ—¢å®šã§è¡¨ç¤ºã€`--no-display` ã§ä¿å­˜ã®ã¿

## å¯¾å¿œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

- ResNet18
- ResNet34
- ResNet50
- ResNet101
- ResNet152

## ä¾å­˜é–¢ä¿‚

- PyTorch / torchvision
- Transformers / Accelerate
- NumPy
- Matplotlibï¼ˆå¯è¦–åŒ–ï¼‰
- OpenCVï¼ˆç”»åƒå‡¦ç†ï¼‰
- TensorBoardXï¼ˆãƒ­ã‚°å‡ºåŠ›ï¼‰

`pyproject.toml` ã«å®šç¾©æ¸ˆã¿ã§ã™ã€‚`uv sync` ã§ç’°å¢ƒæ§‹ç¯‰ã§ãã¾ã™ã€‚

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã® `LICENSE` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## Acknowledgements

This project includes code from:
"Attention Branch Network: Learning of Attention Mechanism for Visual Explanation"  
by Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi,  
licensed under the MIT License.  
Original repository: [https://github.com/machine-perception-robotics-group/attention_branch_network](https://github.com/machine-perception-robotics-group/attention_branch_network)
